{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Concatenating-data\" data-toc-modified-id=\"Concatenating-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Concatenating data</a></span></li><li><span><a href=\"#Finding-and-concatenating-data\" data-toc-modified-id=\"Finding-and-concatenating-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Finding and concatenating data</a></span></li><li><span><a href=\"#Merge-data\" data-toc-modified-id=\"Merge-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Merge data</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining data\n",
    "    - Data may not always come in 1 huge file\n",
    "        - 5 million row dataset may be broken into 5 separate datasets\n",
    "        - Easier to store and share\n",
    "        - May have new data for each day\n",
    "    - Important to be able to combine then clean, or vice versa\n",
    "- pd.concat()\n",
    "        In [1]: concatenated = pd.concat([weather_p1, weather_p2])\n",
    "        In [2]: print(concatenated)\n",
    "             date         element  value\n",
    "        0   2010-01-30  tmax    27.8\n",
    "        1   2010-01-30  tmin     14.5\n",
    "        0   2010-02-02  tmax    27.3\n",
    "        1   2010-02-02  tmin     14.4\n",
    "    - 純粹照順序來，如果 dataset 間的排序不同則無法對齊\n",
    "    - index label won't change in the new dataset\n",
    "    - ignore_index=True, to create new index\n",
    "             In [4]: pd.concat([weather_p1, weather_p2], ignore_index=True)\n",
    "            Out[4]:\n",
    "                     date     element  value\n",
    "            0  2010-01-30  tmax     27.8\n",
    "            1  2010-01-30  tmin     14.5\n",
    "            2  2010-02-02  tmax     27.3\n",
    "            3  2010-02-02  tmin     14.4\n",
    "    - axis=1, concatenate data column-wise\n",
    "        - default axis=0, row-wise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jupyterthemes.jtplot as jtplot\n",
    "%matplotlib inline\n",
    "jtplot.style(theme='onedork')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"uber = pd.read_csv('exercise/nyc_uber_2014.csv', chunksize=99, index_col=0)\\npart_x = 1\\nfor part in uber:\\n    if part_x == 1:\\n        uber1 = part\\n        uber1.to_csv('exercise/nyc_uber_2014_04.csv')\\n    elif part_x == 2:\\n        uber2 = part\\n        uber2.to_csv('exercise/nyc_uber_2014_05.csv')\\n    elif part_x == 3:\\n        uber3 = part\\n        uber3.to_csv('exercise/nyc_uber_2014_06.csv')\\n    part_x += 1\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate dataset in part\n",
    "'''uber = pd.read_csv('exercise/nyc_uber_2014.csv', chunksize=99, index_col=0)\n",
    "part_x = 1\n",
    "for part in uber:\n",
    "    if part_x == 1:\n",
    "        uber1 = part\n",
    "        uber1.to_csv('exercise/nyc_uber_2014_04.csv')\n",
    "    elif part_x == 2:\n",
    "        uber2 = part\n",
    "        uber2.to_csv('exercise/nyc_uber_2014_05.csv')\n",
    "    elif part_x == 3:\n",
    "        uber3 = part\n",
    "        uber3.to_csv('exercise/nyc_uber_2014_06.csv')\n",
    "    part_x += 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 4)\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  4/1/2014 0:11:00  40.7690 -73.9549  B02512\n",
      "1  4/1/2014 0:17:00  40.7267 -74.0345  B02512\n",
      "2  4/1/2014 0:21:00  40.7316 -73.9873  B02512\n",
      "3  4/1/2014 0:28:00  40.7588 -73.9776  B02512\n",
      "4  4/1/2014 0:33:00  40.7594 -73.9722  B02512\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  4/1/2014 0:11:00  40.7690 -73.9549  B02512\n",
      "0  5/1/2014 0:02:00  40.7521 -73.9914  B02512\n",
      "0  6/1/2014 0:00:00  40.7293 -73.9920  B02512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "uber1 = pd.read_csv('exercise/nyc_uber_2014_04.csv', index_col=0)\n",
    "uber2 = pd.read_csv('exercise/nyc_uber_2014_05.csv', index_col=0)\n",
    "uber3 = pd.read_csv('exercise/nyc_uber_2014_06.csv', index_col=0)\n",
    "\n",
    "\n",
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1, uber2, uber3])\n",
    "\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "\n",
    "# Print the head of row_concat\n",
    "print(row_concat.head())\n",
    "print(row_concat.loc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1952, 6)\n",
      "         Date  Day  type_country  counts status country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "ebola = pd.read_csv('exercise/ebola.csv')\n",
    "\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], \n",
    "                     var_name='type_country', value_name='counts')\n",
    "status_country = pd.DataFrame()\n",
    "status_country['status'] = ebola_melt['type_country'].str.split('_').str.get(0)\n",
    "status_country['country'] = ebola_melt['type_country'].str.split('_').str.get(1)\n",
    "\n",
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "ebola_tidy = pd.concat([ebola_melt, status_country], axis=1)\n",
    "\n",
    "# Print the shape of ebola_tidy\n",
    "print(ebola_tidy.shape)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and concatenating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concatenating many files\n",
    "    - Leverage Python’s features with data cleaning in pandas\n",
    "    - In order to concatenate DataFrames: \n",
    "        - They must be in a list\n",
    "        - Can individually load if there are a few datasets\n",
    "    - But what if there are thousands?\n",
    "        - Solution: glob function to find files based on a pattern\n",
    "- Globbing\n",
    "    - Pattern matching for file names \n",
    "    - Wildcards: * ?\n",
    "        - *.csv\n",
    "            -  Any csv file:\n",
    "        - file_?.csv\n",
    "            - Any single character\n",
    "    - Returns a list of file names\n",
    "    - Can use this list to load into separate DataFrames\n",
    "- The plan\n",
    "    - Load files from globbing into pandas \n",
    "    - Add the DataFrames into a list \n",
    "    - Concatenate multiple datasets at once\n",
    "            In [1]: import glob\n",
    "            In [2]: csv_files = glob.glob('*.csv')\n",
    "            In [3]: print(csv_files)\n",
    "            ['file5.csv', 'file2.csv', 'file3.csv', 'file1.csv', 'file4.csv']\n",
    "            In [4]: list_data = []\n",
    "            In [5]: for filename in csv_files:\n",
    "               ...:     data = pd.read_csv(filename)\n",
    "               ...:     list_data.append(data)\n",
    "            In [6]: pd.concat(list_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exercise/nyc_uber_2014_04.csv', 'exercise/nyc_uber_2014_05.csv', 'exercise/nyc_uber_2014_06.csv']\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  5/1/2014 0:02:00  40.7521 -73.9914  B02512\n",
      "1  5/1/2014 0:06:00  40.6965 -73.9715  B02512\n",
      "2  5/1/2014 0:15:00  40.7464 -73.9838  B02512\n",
      "3  5/1/2014 0:17:00  40.7463 -74.0011  B02512\n",
      "4  5/1/2014 0:17:00  40.7594 -73.9734  B02512\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = 'exercise/nyc_uber_2014_*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "csv_files.sort()\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1], index_col=0)\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 4)\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  4/1/2014 0:11:00  40.7690 -73.9549  B02512\n",
      "1  4/1/2014 0:17:00  40.7267 -74.0345  B02512\n",
      "2  4/1/2014 0:21:00  40.7316 -73.9873  B02512\n",
      "3  4/1/2014 0:28:00  40.7588 -73.9776  B02512\n",
      "4  4/1/2014 0:33:00  40.7594 -73.9722  B02512\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv, index_col=0)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining data\n",
    "    - Concatenation is not the only way data can be combined\n",
    "        - pd.concat(): 純粹照順序來，如果 dataset 間的排序不同則無法對齊\n",
    "- Merging data\n",
    "    - Similar to joining tables in SQL\n",
    "    - Combine disparate datasets based on common columns\n",
    "            In [1]: pd.merge(left=state_populations  right=state_codes,\n",
    "            ...:               on=None, left_on='state', right_on='name')\n",
    "            Out[1]:\n",
    "                    state      population_2016       name   ANSI\n",
    "            0  California        39250017         California   CA\n",
    "            1       Texas         27862596           Texas   TX\n",
    "            2     Florida         20612439           Florida   FL\n",
    "            3    New York      19745289          New York   NY\n",
    "    - left, right\n",
    "        - 指定ds\n",
    "    -  left_on,  right_on\n",
    "        - 指定 merge col_name\n",
    "- Different types of merges\n",
    "    - One-to-one\n",
    "    - Many-to-one\n",
    "    - Many-to-many\n",
    "    - All use the same function\n",
    "    - Only difference is the DataFrames you are merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     3
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name    lat    long\n",
      "0   DR-1 -49.85 -128.57\n",
      "1   DR-3 -47.15 -126.72\n",
      "2  MSK-4 -48.87 -123.40\n",
      "   ident   site       dated\n",
      "0    619   DR-1  1927-02-08\n",
      "1    622   DR-1  1927-02-10\n",
      "2    734   DR-3  1939-01-07\n",
      "3    735   DR-3  1930-01-12\n",
      "4    751   DR-3  1930-02-26\n",
      "5    752   DR-3         NaN\n",
      "6    837  MSK-4  1932-01-14\n",
      "7    844   DR-1  1932-03-22\n",
      "    name    lat    long  ident   site       dated\n",
      "0   DR-1 -49.85 -128.57    619   DR-1  1927-02-08\n",
      "1   DR-1 -49.85 -128.57    622   DR-1  1927-02-10\n",
      "2   DR-1 -49.85 -128.57    844   DR-1  1932-03-22\n",
      "3   DR-3 -47.15 -126.72    734   DR-3  1939-01-07\n",
      "4   DR-3 -47.15 -126.72    735   DR-3  1930-01-12\n",
      "5   DR-3 -47.15 -126.72    751   DR-3  1930-02-26\n",
      "6   DR-3 -47.15 -126.72    752   DR-3         NaN\n",
      "7  MSK-4 -48.87 -123.40    837  MSK-4  1932-01-14\n"
     ]
    }
   ],
   "source": [
    "site = {'name':['DR-1',  'DR-3', 'MSK-4'], 'lat':[-49.85, -47.15, -48.87],\n",
    "        'long': [-128.57, -126.72, -123.40]}\n",
    "site_ds = pd.DataFrame(site)\n",
    "visited =  {'ident':[619, 622, 734, 735, 751, 752, 837, 844],\n",
    "            'site':['DR-1', 'DR-1', 'DR-3', 'DR-3', 'DR-3', 'DR-3', 'MSK-4', 'DR-1'],\n",
    "        'dated': ['1927-02-08', '1927-02-10', '1939-01-07', '1930-01-12',\n",
    "                   '1930-02-26', 'NaN', '1932-01-14', '1932-03-22']}\n",
    "visited_ds = pd.DataFrame(visited)\n",
    "\n",
    "# Merge the DataFrames: m2o\n",
    "m2o = pd.merge(left=site_ds, right=visited_ds, left_on='name', right_on='site')\n",
    "\n",
    "# Print m2o\n",
    "print(site_ds)\n",
    "print(visited_ds)\n",
    "print(m2o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
